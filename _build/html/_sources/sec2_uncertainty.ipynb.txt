{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Confronting Uncertainty \n",
    "\n",
    "## 2.1: Brownian Motion Misspecification\n",
    "\n",
    "\n",
    "Under a baseline probability specification, $W  := \\{ W_t : t \\ge 0\\}$ is a multivariate standard Brownian motion,  and ${\\mathfrak F}  := \\{ {\\mathfrak F}_t : t \\ge 0\\}$ is the corresponding information filtration with $\\mathfrak{F}_t$ generated information that is realized between dates zero and t. This information includes the Brownian increments that have been realized up to date $t$.  Initially, we let ${\\mathfrak F}$ be the Brownian filtration, but we subsequently will augment this filtration to include realizations of jumps that will influence technology and damages induced by climate change.\n",
    "\n",
    "\n",
    "As is familiar from derivative claims pricing, positive martingales with expectations equal to one parameterize changes in probability measures.  From Girsanov theory, such martingales can be characterized  by their implied drift distortions.  In particular, under the martingale change in the probability measure, process $W  := \\{ W_t : t \\ge 0\\}$ instead has a drift $H := \\{ H_t : t \\ge 0\\}$.  \n",
    "\n",
    "Suppose that the state vector process $X$ has a local mean increment $\\mu_x(X_t,A_t)dt $ and stochastic increment $\\sigma_x(X_t, A_t) dW_t$, where $A_t$ is a decision or action taken at time $t$. Throughout the essay we let lower-case variables capture potential realizations of random vectors. The realizations of the state vector $X_t$ reside in a state space ${\\mathcal X}.$ For a value function,  ${\\widehat V},$\n",
    "  the drift or local mean of ${\\widehat V} (X)$ is given by\\footnote{\n",
    "We use the notation $\\frac{\\partial {\\widehat V} }{\\partial x}(x)$ to denote a column vector of derivatives with respect to the column vector $x$  and  $\\frac{\\partial {\\widehat V} }{\\partial x'}(x)$ to be the corresponding row vectors of derivatives with respect to the row vector $x'.$} \n",
    "\n",
    "\\begin{equation} \n",
    "\\frac{\\partial {\\widehat V} }{\\partial x'}(X_t)\\mu_x(X_t,A_t) + \\frac 1 2 {\\rm trace}\\left[\\sigma_x(X_t,A_t)' \\frac{\\partial^2 {\\widehat V} }{\\partial x \\partial x'}(X_t)\\sigma_x(X_t,A_t)\\right] .\n",
    "\\end{equation} \n",
    "\n",
    "In this equation, we omit time dependence and think of  ${\\widehat V} $ as  the value function for an infinite horizon discounted problem.\n",
    "\n",
    "<!-- Under a baseline probability specification, $W  \\doteq \\{ W_t : t \\ge 0\\}$ is a multivariate standard Brownian motion. \n",
    "\n",
    "We partition the vector of Brownian motion into four subvectors as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "dW_t = \\begin{bmatrix} dW_t^k \\cr dW_t^r \\cr dW_t^y \\cr dW_t^n \\cr \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where the first component consists of the technology shocks, the second component consists of the R\\&D shocks and the third component contains the climate change shocks.  -->\n",
    "\n",
    "Formula (1) captures the time increment to risk confronted by the decision-maker.  \n",
    "\n",
    "The decision-maker entertains possible misspecification uncertainty by replacing formula (1) with the solution\n",
    "\n",
    "\\begin{align*} \n",
    "\\min_{h} \\frac{\\partial {\\widehat V} }{\\partial x'}(x) \\left[\\mu_x(x,a) + \\sigma_x(x,a) h \\right] + \\frac 1 2 {\\rm trace}\\left[\\sigma_x(x,a)' \\frac{\\partial^2 {\\widehat V} }{\\partial x \\partial x'}(x)\\sigma_x(x,a)\\right] + \\frac{\\xi}{2} h'h \n",
    "\\end{align*}\n",
    "\n",
    "for a penalty parameter $\\xi$.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The minimization captures a form of uncertainty aversion, analogous to risk aversion,  since the minimizing objective will be less than or equal to (1). The penalty parameter $\\xi$ restrains the concern for robustness to model misspecification.  The quadratic penalty in $h$ is a local measure of ``relative entropy''  or Kulback--Leibler divergence.   A limiting choice of $\\xi \\approx \\infty$ implies a minimizing choice of  $h=0$ with an implied contribution given by (1).  Since the minimization problem is quadratic in $h$, the minimizer is\n",
    "\n",
    "\\begin{equation} \n",
    "h^* = - \\frac 1 {\\xi} \\sigma_x(x,a)'\\frac{\\partial {\\widehat V} }{\\partial x}(x)\n",
    "\\end{equation} \n",
    "\n",
    "with a minimized objective:\n",
    "\n",
    "\\begin{align*} \n",
    "\\frac{\\partial {\\widehat V} }{\\partial x'}(x) \\mu_x(x,a) + \\frac 1 2 {\\rm trace}\\left[\\sigma_x(x,a)' \\frac{\\partial^2 {\\widehat V} }{\\partial x \\partial x'}(x)\\sigma_x(x,a)\\right] \n",
    "- \\frac 1 {2 \\xi} \\frac{\\partial {\\widehat V} }{\\partial x'}(x)\\sigma_x(x,a)\\sigma_x(x,a)'\\frac{\\partial {\\widehat V} }{\\partial x}(x) .\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Notice that the minimizing drift, $h^*,$ is potentially state dependent.  When $\\sigma$ depends on the action $a$, the drift of interest for valuation and interpretation depends on the maximizing action $a$ expressed as a function of the state.  The drift vector, $h^*,$ has larger entries where the   value function is more adversely exposed to the Brownian increments.  Smaller values of $\\xi$ result in drift adjustments with larger magnitudes.\\footnote{While this looks obvious from formula \\eqref{drift_distort}, it is a bit more subtle because the value function implicitly depends on $\\xi$.} \n",
    "\n",
    "\n",
    "> While we motivated the  adjustment to  an HJB equation in terms of robustness, it may also be viewed as a risk adjustment  in conjunction with recursive utility.  Consider the local counterpart or small $\\epsilon$ counterpart to the risk adjustment:\n",
    "\\begin{align} \n",
    "& \\frac 1 {\\epsilon } \\left( \\frac 1 {1 -\\gamma}  \\log {\\mathbb E} \\left[ \\exp \\left( (1- \\gamma) {\\widehat V}(X_{t+\\epsilon})  \\right) \\mid {\\mathfrak F}_t \\right] - {\\widehat V}(X_t)\\right) \\cr &  =  \\frac 1 {\\epsilon (1 -\\gamma) }\\log {\\mathbb E} \\left[ \\exp \\left( (1 - \\gamma) \\left({\\widehat V}(X_{t+\\epsilon}) - {\\widehat V}(X_t) \\right) \\right) \\mid {\\mathfrak F}_t \\right] .\n",
    "\\end{align}\n",
    "This exponential risk adjustment of the continuation value induces a local log-normal type of adjustment given by \n",
    "\\begin{align*} \n",
    "(1- \\gamma)  \\frac {\\partial {\\widehat V}(x)}  {\\partial x'}\\sigma(x,a) \\sigma(x,a)' \\frac {\\partial {\\widehat V}(x)}{\\partial x} ,\n",
    "\\end{align*}\n",
    "where the term multiplying $1-\\gamma$ is the local variance of the continuation value process ${\\widehat V}(X)$.  Setting $ \\gamma - 1 = \\frac 1 {\\xi}$ gives a mathematical equivalence between robustness and risk considerations, although the rationale for the two adjustments is very different.   We will eventually show how to embed this into a full recursive utility specification preferences.  The resulting equivalence is a direct extension of a well-known result from risk-sensitive control theory. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> As \\cite{AndersonHansenSargent:2003} emphasize, the negative implied drift distortions from a planner's problem are also the local  shadow prices for concerns about misspecification.  While they featured the case in which these shadow prices are also pertinent for competitive financial markets, the same insight carries over to social valuation in the presence of externalities that induce a wedge between market prices and social counterparts.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.2: Jump misspecification\n",
    "\n",
    "We suppose there is a discrete set of jump states $\\mathcal{Z}$. Let $z$ denote a realized value in the set $\\mathcal{Z}$. Let $\\mathcal{J}$ denote a state-dependent jump intensity, and let $\\pi(\\tilde{z} \\mid x, z), z \\in \\mathcal{Z}$ give the jump distribution conditioned on a jump when $X_t=x$ in discrete state $Z_t=z$. Recall that the jump intensity, $\\mathcal{J}$, implies an approximate jump probability, $\\epsilon \\mathcal{J}$, over a small time increment, $\\epsilon$. Following a jump, $x$ changes as does the value function. For each choice of $\\tilde{z}$, $x$ jumps to the $\\tilde{x}(\\tilde{z})$, where $\\tilde{z}$ is uncertain prior to the jump. The baseline probabilities are $\\pi(\\tilde{z} \\mid x, z)$ for $z \\in \\mathcal{Z}$, where\n",
    "\n",
    "$$\n",
    "\\sum_{\\mathcal{Z}} \\pi(\\tilde{z} \\mid x, z)=1 .\n",
    "$$\n",
    "\n",
    "With these jumps, a value function shifts from $\\hat{V}(x, z)$ to $\\tilde{V}[\\tilde{x}(\\tilde{z}), \\tilde{z}]$. The jump process contributes the following term to the drift of $\\widehat{V}(X, Z)$ :\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{J}(x, z) \\sum_{\\tilde{z} \\in \\mathcal{Z}}[\\tilde{V}[\\tilde{x}(\\tilde{z}), \\tilde{z}]-\\hat{V}(x, z)] \\pi(\\tilde{z} \\mid x, z)\n",
    "\\end{equation}\n",
    "  \n",
    "capturing the jump risk contribution to the decision problem.\n",
    "\n",
    "To capture potential misspecification, we introduce a non-negative function $f$ where the altered jump distribution is\n",
    "\n",
    "$$\n",
    "\\frac{f(\\tilde{z} \\mid x, z) \\pi(\\tilde{z} \\mid x, z)}{\\tilde{f}(x, z)}\n",
    "$$\n",
    "\n",
    "and intensity $\\mathcal{J}(x, z) \\bar{f}(x, z)$ where \n",
    "\n",
    "$$\n",
    "\\bar{f}(x, z)=\\sum_{\\bar{z} \\in \\mathcal{Z}} f(\\tilde{z} \\mid x, z) \\pi(\\tilde{z} \\mid x, z) .\n",
    "$$\n",
    "\n",
    "To restrain the exploration of potential misspecification, we introduce a convex cost:\n",
    "\n",
    "$$\n",
    "\\xi \\mathcal{J}(x, z) \\sum_{\\tilde{z} \\in \\mathcal{Z}}[1-f(\\tilde{z} \\mid x, z)+f(\\tilde{z} \\mid x, z) \\log f(\\tilde{z} \\mid x, z)] \\pi(\\tilde{z} \\mid x, z)\n",
    "$$\n",
    "\n",
    "The term multiplying $\\xi$ is a local (in time) measure of relative entropy or Kullbackâ€“Leibler\n",
    "divergence applicable to jump processes\n",
    "\n",
    "To confront misspecification, we solve:\n",
    "\n",
    "\\begin{align*} \n",
    "\\min _{f \\geqslant 0} & \\mathcal{J}(x, z) \\sum_{\\tilde{z} \\in \\mathcal{Z}}[\\tilde{V}[\\tilde{x}(\\tilde{z}), \\tilde{z}]-\\hat{V}(x, z)] f(\\tilde{z} \\mid x, z) \\pi(\\tilde{z} \\mid x, z) \\\\\n",
    "& +\\xi \\mathcal{J}(x, z) \\sum_{\\tilde{z} \\in \\mathcal{Z}}[1-f(\\tilde{z} \\mid x, z)+f(\\tilde{z} \\mid x, z) \\log f(\\tilde{z} \\mid x, z)] \\pi(\\tilde{z} \\mid x, z) .\n",
    "\\end{align*}\n",
    "\n",
    "The above minimization problem has a quasi-analytical solution:\n",
    "\n",
    "\\begin{align}\n",
    "f^*( {\\tilde z}  \\mid x, z )  = \\exp \\left( - {\\frac{1}{\\xi}} \\left( {\\widetilde  V} [{\\tilde x}({\\tilde z}), {\\tilde z} ] - {\\widehat V} (x, z) \\right) \\right), \n",
    "\\end{align}\n",
    "\n",
    "with a minimized objective:\n",
    "\\begin{equation}\n",
    "\\xi {\\mathcal J}(x, z) \\left[ 1 -  \\sum_{{\\tilde z } \\in {\\mathcal Z}}   \\exp \\left( - {\\frac{1}{\\xi}} \\left( {\\widetilde  V} [{\\tilde x}({\\tilde z}), {\\tilde z} ] - {\\widehat V} (x, z) \\right) \\right) \\right] \\pi({\\tilde z} \\mid x, z)  ,\n",
    "\\end{equation}\n",
    "\n",
    "which we use in place of (1).  \n",
    "\n",
    "## 2.3 Incorporating ambiguity aversion\n",
    "\n",
    "\n",
    "Imagine there are alternative models of different components of the dynamics.  We follow \\cite{HansenMiao:2018} by supposing that the drift $\\mu(x, z, a \\mid \\theta)$ depends on an unknown parameter $\\theta$ residing in a set $\\Theta.$   The parameter,\n",
    "$\\theta,$ could index one of a discrete set of alternative models or depict a unknown  parameter vector.  The decision-maker has a baseline probability $d P_t(\\theta)$ for each time instant, $t$,  and makes an adjustment for ambiguity by solving \n",
    "\\begin{align*}\n",
    "\\min_{q,\\, \\int_\\Theta q(\\theta) dP_t(\\theta)  = 1} \\hspace{.3cm} &\\frac{\\partial {\\widehat V} }{\\partial x'}(x, z) \\int_\\Theta \\mu_x(x, z, a \\mid \\theta) q(\\theta)  d P_t(\\theta)  \\cr & + \\chi \\int_\\Theta  q(\\theta) \\log q(\\theta) d P_t(\\theta) ,\n",
    "\\end{align*}\n",
    "where $\\chi$ is a penalty parameter. \n",
    "\n",
    "\n",
    "This problem is known to have a solution that entails exponential tilting as a function of the drift of the value function for alternative values of $\\theta$: \n",
    "\n",
    "\\begin{equation*}\n",
    "q^*_t({\\tilde \\theta}) = \\frac {\\exp\\left( - \\frac 1 {\\chi} \\frac{\\partial {\\widehat V} }{\\partial x'}(x, z)  \\mu_x (x, z, a \\mid {\\tilde \\theta}) \\right)}{ \\int_\\Theta \\exp\\left( - \\frac 1 {\\chi} \\frac{\\partial {\\widehat V} }{\\partial x'}(x, z)  \\mu_x\\left(x, z, a \\mid \\theta\\right)\\right) d P_t(\\theta) } .\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "The minimized objective is\n",
    "\\begin{equation*}\n",
    "\\chi \\log \\int_\\Theta  \\exp\\left( - \\frac 1 {\\chi} \\frac{\\partial {\\widehat V} }{\\partial x'}(x, z)  \\mu_x(x, z, a \\mid \\theta)\\right) d P_t(\\theta) .\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Notice that this formulation implies an exponential  adjustment for model ambiguity concerns.\\footnote{As noted by \\cite{HansenMiao:2018}, this exponential adjustment can equivalently be viewed as a continuous-time version of a smooth ambiguity adjustment.}  We allow the baseline probability to be time dependent to allow for recursive learning, although we will abstract from this learning in our application.  \n",
    "\n",
    "Problem 3 and  Problem 2 show a notable similarity.  The smooth ambiguity model applies to Brownian uncertainty, and the objective of interest is the local evolution of the value function.  In the case of jump uncertainty,  this is replaced by the intensity times the difference between the post-jump and pre-jump value functions.  The counterpart to $\\chi$ for the smooth ambiguity adjustment is the intensity times $\\xi.$    \n",
    "\n",
    "The relative density $q$ in Problem 3 plays a role analogous to $ {f}/{{\\bar f}}$ in Problem 2 when deducing the worst-case distribution.  With this mapping, the two robustness adjustments are mathematically equivalent.  As we noted, however, the required  specification of the intensity  introduces an additional source of potential misspecification for the case of jump uncertainty.     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
